{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f30b4f6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ===========================\n",
    "# 1. 配置参数\n",
    "# ===========================\n",
    "CONFIG = {\n",
    "    'seed': 42,\n",
    "    'batch_size': 64,  \n",
    "    'num_epochs': 30,  # 继续训练 30 轮\n",
    "    'learning_rate': 1e-4, # 继续训练时 LR 可以稍微调小一点，或者保持 3e-4 也可以，这里稍微保守一点用 1e-4\n",
    "    'input_size': 224,\n",
    "    'num_classes': 6,\n",
    "    'device': torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    'data_dir': '/kaggle/input/neu-image-emotion-classification/fer_data/fer_data/train',\n",
    "    'test_dir': '/kaggle/input/neu-image-emotion-classification/fer_data/fer_data/test'\n",
    "}\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(CONFIG['seed'])\n",
    "\n",
    "# ===========================\n",
    "# 2. 数据处理 (保持不变)\n",
    "# ===========================\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.Resize((CONFIG['input_size'], CONFIG['input_size'])),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(20),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.2, 0.2), scale=(0.8, 1.2)),\n",
    "        transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        transforms.RandomErasing(p=0.5, scale=(0.02, 0.1))\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.Resize((CONFIG['input_size'], CONFIG['input_size'])),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "full_dataset = datasets.ImageFolder(root=CONFIG['data_dir'])\n",
    "train_size = int(0.85 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "class TransformedDataset(Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n",
    "\n",
    "train_set = TransformedDataset(train_dataset, transform=data_transforms['train'])\n",
    "val_set = TransformedDataset(val_dataset, transform=data_transforms['val'])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_set, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# ===========================\n",
    "# 3. Mixup 工具函数\n",
    "# ===========================\n",
    "def mixup_data(x, y, alpha=1.0, use_cuda=True):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    batch_size = x.size(0)\n",
    "    if use_cuda:\n",
    "        index = torch.randperm(batch_size).cuda()\n",
    "    else:\n",
    "        index = torch.randperm(batch_size)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "# ===========================\n",
    "# 4. 模型构建与权重加载\n",
    "# ===========================\n",
    "class_counts = { 'Angry': 3963, 'Fear': 4097, 'Happy': 7192, 'Neutral': 4959, 'Sad': 4862, 'Surprise': 3202 }\n",
    "class_names = full_dataset.classes\n",
    "counts = [class_counts[name] for name in class_names]\n",
    "weights = [max(counts) / c for c in counts]\n",
    "class_weights = torch.FloatTensor(weights).to(CONFIG['device'])\n",
    "\n",
    "def build_model():\n",
    "    model = models.resnet34(pretrained=True) \n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(p=0.5),\n",
    "        nn.Linear(num_ftrs, CONFIG['num_classes'])\n",
    "    )\n",
    "    return model.to(CONFIG['device'])\n",
    "\n",
    "model = build_model()\n",
    "\n",
    "# --- 【关键修改】加载之前的最佳权重 ---\n",
    "checkpoint_path = 'best_model_fer.pth'\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"Found checkpoint '{checkpoint_path}'. Loading weights to resume training...\")\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "else:\n",
    "    print(f\"No checkpoint found at '{checkpoint_path}'. Starting training from scratch (Pretrained ImageNet).\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=1e-2)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG['num_epochs'], eta_min=1e-6)\n",
    "\n",
    "# ===========================\n",
    "# 5. 训练循环 (增强版打印)\n",
    "# ===========================\n",
    "\n",
    "# 在开始训练前，先评估一下当前模型在验证集上的表现，作为基准 best_acc\n",
    "print(\"Evaluating initial baseline accuracy...\")\n",
    "model.eval()\n",
    "initial_corrects = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs, labels = inputs.to(CONFIG['device']), labels.to(CONFIG['device'])\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        initial_corrects += torch.sum(preds == labels.data)\n",
    "best_acc = initial_corrects.double() / len(val_set)\n",
    "print(f\"Initial Baseline Val Acc: {best_acc:.4f}\")\n",
    "\n",
    "print(\"\\nStart Resumed Training...\")\n",
    "print(f\"{'Epoch':^10} | {'Train Loss':^12} | {'Train Acc':^12} | {'Val Loss':^12} | {'Val Acc':^12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for epoch in range(CONFIG['num_epochs']):\n",
    "    # --- Training Phase ---\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_corrects = 0.0 # 用于计算训练集准确率\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(CONFIG['device']), labels.to(CONFIG['device'])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Mixup\n",
    "        inputs, targets_a, targets_b, lam = mixup_data(inputs, labels, alpha=0.4)\n",
    "        inputs, targets_a, targets_b = map(torch.autograd.Variable, (inputs, targets_a, targets_b))\n",
    "        \n",
    "        outputs = model(inputs)\n",
    "        loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        # 计算 Mixup 下的训练准确率 (软准确率)\n",
    "        # 如果预测结果等于 targets_a，得分 lam；如果等于 targets_b，得分 1-lam\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        part_a = (preds == targets_a).float() * lam\n",
    "        part_b = (preds == targets_b).float() * (1 - lam)\n",
    "        train_corrects += (part_a + part_b).sum().item()\n",
    "\n",
    "    scheduler.step()\n",
    "    \n",
    "    epoch_train_loss = running_loss / len(train_set)\n",
    "    epoch_train_acc = train_corrects / len(train_set)\n",
    "    \n",
    "    # --- Validation Phase ---\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_corrects = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(CONFIG['device']), labels.to(CONFIG['device'])\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # 计算验证 Loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            # 计算验证 Acc\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "    epoch_val_loss = val_loss / len(val_set)\n",
    "    epoch_val_acc = val_corrects.double() / len(val_set)\n",
    "\n",
    "    # --- Print Stats ---\n",
    "    print(f\"Epoch {epoch+1:02d}/{CONFIG['num_epochs']} | {epoch_train_loss:.4f}       | {epoch_train_acc:.4f}       | {epoch_val_loss:.4f}       | {epoch_val_acc:.4f}\")\n",
    "\n",
    "    # --- Save Best Model ---\n",
    "    if epoch_val_acc > best_acc:\n",
    "        best_acc = epoch_val_acc\n",
    "        torch.save(model.state_dict(), 'best_model_fer.pth')\n",
    "        print(f\"  >>> New Best Model Saved! Acc: {best_acc:.4f}\")\n",
    "\n",
    "print(f\"\\nTraining Complete. Best Validation Accuracy: {best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4528de50",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ===========================\n",
    "# 提交代码 (Inference & Submission) - 包含标签映射修正\n",
    "# ===========================\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 1. 定义测试集 Dataset\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        # 必须排序，保证文件名和预测结果一一对应\n",
    "        self.img_names = sorted(os.listdir(img_dir)) \n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_names[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB') # 确保转为 RGB\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, img_name\n",
    "\n",
    "# 2. 加载模型\n",
    "# 确保这里的 build_model() 和你训练时定义的网络结构一致 (ResNet34)\n",
    "model = build_model()\n",
    "\n",
    "# 加载你刚才训练好的权重\n",
    "model_path = 'best_model_fer.pth' \n",
    "if os.path.exists(model_path):\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    print(f\"Loaded model weights from {model_path}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Error: Model file {model_path} not found!\")\n",
    "    \n",
    "model.eval()\n",
    "\n",
    "# 3. 定义测试转换 (与验证集保持一致)\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((CONFIG['input_size'], CONFIG['input_size'])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 4. 准备 DataLoader\n",
    "test_dataset = TestDataset(CONFIG['test_dir'], transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "# ===========================\n",
    "# 【核心修正】定义标签映射字典\n",
    "# ===========================\n",
    "# 你的模型训练顺序 (ImageFolder 默认字母序): \n",
    "# 0:Angry, 1:Fear, 2:Happy, 3:Neutral, 4:Sad, 5:Surprise\n",
    "#\n",
    "# 题目要求的提交顺序:\n",
    "# 0:Angry, 1:Fear, 2:Happy, 3:Sad, 4:Surprise, 5:Neutral\n",
    "#\n",
    "# 映射逻辑 (模型输出 -> 题目要求):\n",
    "idx_map = {\n",
    "    0: 0, # Angry -> Angry\n",
    "    1: 1, # Fear -> Fear\n",
    "    2: 2, # Happy -> Happy\n",
    "    3: 5, # Neutral (模型输出3) -> 题目要求 5\n",
    "    4: 3, # Sad (模型输出4)     -> 题目要求 3\n",
    "    5: 4  # Surprise (模型输出5)-> 题目要求 4\n",
    "}\n",
    "\n",
    "predictions = []\n",
    "filenames = []\n",
    "\n",
    "print(\"Starting Inference with Label Remapping...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, names in test_loader:\n",
    "        images = images.to(CONFIG['device'])\n",
    "        \n",
    "        # --- TTA (Test Time Augmentation) ---\n",
    "        # 1. 原图预测\n",
    "        output1 = model(images)\n",
    "        # 2. 水平翻转图预测\n",
    "        output2 = model(torch.flip(images, [3]))\n",
    "        \n",
    "        # 3. 取平均\n",
    "        avg_output = (output1 + output2) / 2.0\n",
    "        \n",
    "        # 获取预测结果 (0-5)\n",
    "        _, preds = torch.max(avg_output, 1)\n",
    "        \n",
    "        # 转为列表以便处理\n",
    "        preds_list = preds.cpu().numpy().tolist()\n",
    "        \n",
    "        # --- 【关键步骤】应用映射修正 ---\n",
    "        # 将模型预测出的索引，翻译成题目要求的索引\n",
    "        remapped_preds = [idx_map[p] for p in preds_list]\n",
    "        \n",
    "        predictions.extend(remapped_preds)\n",
    "        filenames.extend(names)\n",
    "\n",
    "# 5. 生成提交文件\n",
    "# 确保列名为 ID 和 Emotion (与你之前的截图一致)\n",
    "df_sub = pd.DataFrame({\n",
    "    'ID': filenames, \n",
    "    'Emotion': predictions\n",
    "})\n",
    "\n",
    "print(df_sub.head())\n",
    "\n",
    "# 保存文件\n",
    "df_sub.to_csv('submission.csv', index=False)\n",
    "print(\"Submission saved successfully! (Label mapping applied)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
